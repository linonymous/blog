<div class="post-body entry-content float-container" id="post-body-496506970456222005">
<div dir="ltr" style="text-align: left;" trbidi="on">
<h3 style="text-align: justify;">
Introduction</h3>
<div style="text-align: justify;">
I have been working on Golang for quite a time now. I explored a lot of features. The few that caught up my eye was 'Scalability' &amp; 'Concurrency'. Scalability &amp; Concurrency have been some of the major objectives behind the design of Golang. Let's dive in a bit.</div>
<h3 style="text-align: justify;">
Threads </h3>
<div style="text-align: justify;">
A thread is the unit of execution within a process. A process can have anywhere from just one thread to many threads. On a machine, we have multiple processes running and in these processes, we have independent or dependent threads aggregating computations. </div>
<div style="text-align: justify;">
<br/></div>
<div style="text-align: justify;">
Contextually, these threads are further broken down into two types, namely <b>User-Level Threads </b>and <b>Kernel Level Threads</b>. The basic difference between these threads is that the kernel-level threads are managed, operated, and scheduled by the operating system(kernel), and user-level threads are managed, operated, and scheduled by the application layer. </div><div style="text-align: justify;"><br/></div><div style="text-align: justify;">Just to have more understanding about them, let's list down the advantages and disadvantages!</div>
<div style="text-align: justify;">
<br/></div>
<div style="text-align: justify;"><b>
Advantages of Kernel Threads</b></div>
<div>
<ul style="text-align: left;">
<li style="text-align: justify;">The kernel knows the whereabouts of kernel threads. Thus, Kernel can schedule these threads optimally, i.e. the scheduler can give priority to a process with a large number of threads over the process with comparatively fewer threads.</li>
<li style="text-align: justify;">Kernel threads are secure and are managed by native OS.</li>
</ul>
<div style="text-align: justify;"><b>
Disadvantages of Kernel Threads</b></div>
</div>
<div>
<ul style="text-align: left;">
<li style="text-align: justify;">Kernel threads are very inefficient, these threads take a lot of time during a context switch. It involves changing a large set of processor registers that define the current memory map and permissions. It also evicts some or all of the processor cache.</li>
<li style="text-align: justify;">Kernel threads are very slow. These threads are spawned by the kernel using system calls, which are hefty when it comes to execution speed. Thus, kernel threads are slow to start/stop.</li>
</ul>
<div style="text-align: justify;"><b>
Advantages of User Threads</b></div>
</div>
<div>
<ul style="text-align: left;">
<li style="text-align: justify;">Well, well, first thing first, you can implement your own user threads, even when the native OS does not support any concurrency. </li>
<li style="text-align: justify;">User threads are comparatively very cheap to spawn and consume less memory than the kernel threads. Creating a new thread, switching between threads &amp; synchronization of these threads are done via procedure calls and have no kernel involvement. Thus, user threads are faster than kernel threads.</li>
</ul><div style="text-align: justify;"><b>Disadvantages of User Threads</b></div><div><ul style="text-align: left;"><li style="text-align: justify;">User-level threads are not optimized for scheduling, the reason being, kernel scheduler is way more optimized than the custom schedulers. </li></ul><div style="text-align: justify;">Looking at the pros and cons, what if we leverage the speed of user-level threads and the scheduling capability of kernel-level threads. Thus, to get the best of both worlds, it's better to multiplex the user-level threads (lightweight, easy to create, but not known to the kernel so poor scheduling) over kernel-level threads (Good at concurrency and scheduling, but inefficient for creation, maintenance &amp; context switch).</div><div style="text-align: justify;"><br/></div><h3 style="text-align: justify;">Underhood Goroutines Concurrency:</h3><div style="text-align: left;"><ul style="text-align: left;"><li style="text-align: justify;">P:   Number of Processors</li><li style="text-align: justify;">M:  Number of threads (os level)</li><li style="text-align: justify;">G:   Number of Goroutines (user level, green threads)</li></ul><div style="text-align: justify;">There are M threads running on P processors and G threads(goroutines) are multiplexed over the M threads(kernel level).</div><div style="text-align: justify;"><br/></div><div style="text-align: justify;">Thus G goroutines need to be scheduled on M os threads which are internally scheduled over P processors. In Golang, the <b>GOMAXPROCS </b>environment variable depicts the number of processors(cores in the system) that will be contributing to the execution of these threads. Note that, <b>GOMAXPROCS</b> set to <b>1</b> means, no parallelism. Given that <b>P &lt;= GOMAXPROCS. </b></div><div style="text-align: justify;"><b><br/></b></div><div style="text-align: justify;">Every processor has a <b>Local Run Queue </b>i.e.<b> LRQ</b>, Goroutines in LRQ are picked up one by one by the scheduler to schedule them on the owner processor of LRQ. Above this, there is a <b>GLOBAL RUN QUEUE </b>i.e. GRQ. GRQ is shared across all threads. As LRQ is local, thus scheduler threads do not need locks over it, i.e. LRQ doesn't need to be synchronized as they are accessed by only one thread.  Whereas, GRQ needs locking as this queue of tasks is shared across all the processor threads. </div><div style="text-align: justify;"><br/></div><div style="text-align: justify;">Whenever the scheduler does not find any thread on LRQ, then it performs<b> thread stealing</b>, which means it randomly accesses the LRQs of other processors (now LRQ needs locking) and steals half of the workload. </div><div style="text-align: justify;"><br/></div><div style="text-align: justify;">If in case, there are no threads to steal from other LRQs, the scheduler gets the workload in GRQ.</div><div style="text-align: justify;"><br/></div><h3 style="text-align: justify;">Advantages of Goroutines:</h3><div><ul style="text-align: left;"><li style="text-align: justify;">Less memory consumption (few kilobytes per goroutine)</li><li style="text-align: justify;">Less setup and teardown cost (user-space threads)</li><li style="text-align: justify;">Context switch cost is less as the scheduling is co-operative and non-preemptive. In cooperative scheduling, there is no concept of the scheduler time slice. In such scheduling, Goroutines yield the control periodically when they are idle or logically blocked in order to tun multiple goroutines concurrently. The switch between goroutines happens only at well-defined points, when an explicit call is made to the Go Runtime Scheduler. And those points are:</li><ul><li style="text-align: justify;">Send/Receive calls over the channels, as it involves the blocking calls</li><li style="text-align: justify;">Go statement, thus it is not guaranteed that the new routine will be scheduled immediately.</li><li style="text-align: justify;">Blocking syscalls for the file or network operations.</li><li style="text-align: justify;">After being stopped for a garbage collection cycle.</li></ul></ul><div style="text-align: justify;"><br/></div></div><div style="text-align: justify;"><b>Fun fact:</b></div><div><ul style="text-align: left;"><li style="text-align: justify;">Considering 2KB size of single goroutine and 8GB of RAM, you can run,</li><ul><li style="text-align: justify;">500 goroutines per 1MB</li><li style="text-align: justify;">500,000 goroutines per 1GB</li><li style="text-align: justify;">4,000,000 goroutines per 8GB</li></ul><li style="text-align: justify;">Given the calculation, on 8GB RAM, we can have around a million goroutines running.</li></ul></div></div></div>
</div>
</div>
</div>